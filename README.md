# simpleGPT
Serve multiple LLMs locally using LangChain + llama-cpp-python, with FastAPI orchestration and Docker isolationâ€”GPU with CUDA recommended for best performance.
